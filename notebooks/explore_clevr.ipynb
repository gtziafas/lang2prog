{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/p300488/lang2prog\n"
     ]
    }
   ],
   "source": [
    " cd /data/p300488/lang2prog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring CLEVR questions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint \n",
    "import numpy as np\n",
    "\n",
    "root = '/data/p300488/lang2prog'\n",
    "\n",
    "# dataset\n",
    "clevr_path = '/data/p300488/datasets/clevr/CLEVR_v1.0'\n",
    "train_questions_path = os.path.join(clevr_path, 'questions/CLEVR_train_questions.json')\n",
    "val_questions_path = os.path.join(clevr_path, 'questions/CLEVR_val_questions.json')\n",
    "test_questions_path = os.path.join(clevr_path, 'questions/CLEVR_test_questions.json')\n",
    "\n",
    "# generalization split\n",
    "cogent_path = '/data/p300488/datasets/clevr/CLEVR_CoGenT_v1.0'\n",
    "gen_trainA_questions_path = os.path.join(cogent_path, 'questions/CLEVR_trainA_questions.json')\n",
    "gen_valA_questions_path = os.path.join(cogent_path, 'questions/CLEVR_valA_questions.json')\n",
    "gen_valB_questions_path = os.path.join(cogent_path, 'questions/CLEVR_valB_questions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CLEVR questions dataset. Size of training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699989\n"
     ]
    }
   ],
   "source": [
    "ds = json.load(open(train_questions_path))['questions']\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the structure of a sample, it contains a program annotation for the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'yes',\n",
      " 'image_filename': 'CLEVR_train_000000.png',\n",
      " 'image_index': 0,\n",
      " 'program': [{'function': 'scene', 'inputs': [], 'value_inputs': []},\n",
      "             {'function': 'filter_size',\n",
      "              'inputs': [0],\n",
      "              'value_inputs': ['large']},\n",
      "             {'function': 'filter_color',\n",
      "              'inputs': [1],\n",
      "              'value_inputs': ['green']},\n",
      "             {'function': 'count', 'inputs': [2], 'value_inputs': []},\n",
      "             {'function': 'scene', 'inputs': [], 'value_inputs': []},\n",
      "             {'function': 'filter_size',\n",
      "              'inputs': [4],\n",
      "              'value_inputs': ['large']},\n",
      "             {'function': 'filter_color',\n",
      "              'inputs': [5],\n",
      "              'value_inputs': ['purple']},\n",
      "             {'function': 'filter_material',\n",
      "              'inputs': [6],\n",
      "              'value_inputs': ['metal']},\n",
      "             {'function': 'filter_shape',\n",
      "              'inputs': [7],\n",
      "              'value_inputs': ['cube']},\n",
      "             {'function': 'count', 'inputs': [8], 'value_inputs': []},\n",
      "             {'function': 'greater_than',\n",
      "              'inputs': [3, 9],\n",
      "              'value_inputs': []}],\n",
      " 'question': 'Are there more big green things than large purple shiny cubes?',\n",
      " 'question_family_index': 2,\n",
      " 'question_index': 0,\n",
      " 'split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "pprint(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'see all the different reasoning primitives and their related concept values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count',\n",
      " 'equal_color',\n",
      " 'equal_integer',\n",
      " 'equal_material',\n",
      " 'equal_shape',\n",
      " 'equal_size',\n",
      " 'exist',\n",
      " 'filter_color[blue]',\n",
      " 'filter_color[brown]',\n",
      " 'filter_color[cyan]',\n",
      " 'filter_color[gray]',\n",
      " 'filter_color[green]',\n",
      " 'filter_color[purple]',\n",
      " 'filter_color[red]',\n",
      " 'filter_color[yellow]',\n",
      " 'filter_material[metal]',\n",
      " 'filter_material[rubber]',\n",
      " 'filter_shape[cube]',\n",
      " 'filter_shape[cylinder]',\n",
      " 'filter_shape[sphere]',\n",
      " 'filter_size[large]',\n",
      " 'filter_size[small]',\n",
      " 'greater_than',\n",
      " 'intersect',\n",
      " 'less_than',\n",
      " 'query_color',\n",
      " 'query_material',\n",
      " 'query_shape',\n",
      " 'query_size',\n",
      " 'relate[behind]',\n",
      " 'relate[front]',\n",
      " 'relate[left]',\n",
      " 'relate[right]',\n",
      " 'same_color',\n",
      " 'same_material',\n",
      " 'same_shape',\n",
      " 'same_size',\n",
      " 'scene',\n",
      " 'union',\n",
      " 'unique'}\n"
     ]
    }
   ],
   "source": [
    "all_primitives = set()\n",
    "for sample in ds:\n",
    "    for node in sample['program']:\n",
    "        _fn = node['function']\n",
    "        _side_input =  '[' + node['value_inputs'][0] + ']' if node['value_inputs'] else ''\n",
    "        all_primitives.add(_fn + _side_input)\n",
    "\n",
    "pprint(all_primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this formalism, the primitives are both concept-aware (``filter_color, filter_size`` etc.), as well as vocabulary-aware (``filter_color[red], filter_color[blue]``, etc ). Let's create a version which decouples specific concept values from the primitives (*vocabulary-agnostic*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count',\n",
      " 'equal_color',\n",
      " 'equal_integer',\n",
      " 'equal_material',\n",
      " 'equal_shape',\n",
      " 'equal_size',\n",
      " 'exist',\n",
      " 'filter_color',\n",
      " 'filter_material',\n",
      " 'filter_shape',\n",
      " 'filter_size',\n",
      " 'greater_than',\n",
      " 'intersect',\n",
      " 'less_than',\n",
      " 'query_color',\n",
      " 'query_material',\n",
      " 'query_shape',\n",
      " 'query_size',\n",
      " 'relate',\n",
      " 'same_color',\n",
      " 'same_material',\n",
      " 'same_shape',\n",
      " 'same_size',\n",
      " 'scene',\n",
      " 'union',\n",
      " 'unique'}\n"
     ]
    }
   ],
   "source": [
    "vocab_agnostic_primitives = set()\n",
    "concept_agnostic_primitives = set()\n",
    "for fn in all_primitives:\n",
    "    f = fn.split('[')[0]\n",
    "    vocab_agnostic_primitives.add(f)\n",
    "    if len(f.split('_')) > 1:\n",
    "        f = f.split('_')[0] if f.split('_')[1] not in ['than', 'integer'] else f\n",
    "    concept_agnostic_primitives.add(f)\n",
    "\n",
    "pprint(vocab_agnostic_primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the most general formalism, without concept-awareness (*concept_agnostic*)\\:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count',\n",
      " 'equal',\n",
      " 'equal_integer',\n",
      " 'exist',\n",
      " 'filter',\n",
      " 'greater_than',\n",
      " 'intersect',\n",
      " 'less_than',\n",
      " 'query',\n",
      " 'relate',\n",
      " 'same',\n",
      " 'scene',\n",
      " 'union',\n",
      " 'unique'}\n"
     ]
    }
   ],
   "source": [
    "pprint(concept_agnostic_primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give some context on different primitive types:\n",
    "\n",
    "   - **Operational** : ``scene``: Initializes a set of objects given RGB image, ``unique``: {n} -> n\n",
    "   \n",
    "   - **Logical**: ``union/intersection``: union / intersection of two sets (outputs of two reasoning branches),\n",
    "   \n",
    "   - **Enumeration**: ``exist``: is a set non-empty?, ``count``: size of set, ``less_than/greater_than/equal_integer``: compares two integers\n",
    "   \n",
    "   - **Visual**: ``filter``: isolate object set based on attribute value, ``query``: ask for an attribute value, ``same``: object set which has same attribute value as given, ``equal``: whether two objects have equal attribute value\n",
    "   \n",
    "   - **Spatial**: ``relate``: object set which has certain spatial relation to given object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Language-to-Program datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Programs with different formalisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the program annotations to a universal format, which also works for GQA dataset. The format breaks down each reasoning primitive as a ``ProgramNode`` object, consisting of a function call (*concept_agnostic*, e.g. ``filter``), a ``concept_input`` field (e.g. ``color``), a ``value_input`` field (e.g. ``[blue]``) and an ``inputs`` field, denoting the index of the reasoning steps whose output is input for the current step. \n",
    "\n",
    "This representation allows us to restructure the program annotations in whatever fashion (*concept_agnostic*, *vocab-agnostic* etc.) we want for different learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typings import *\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEVR_CONCEPTS = [\"color\", \"material\", \"size\", \"shape\"]\n",
    "\n",
    "def formalize_program_annots(ds, concept_list=CLEVR_CONCEPTS):\n",
    "    programs = []\n",
    "    for sample in ds:\n",
    "        _nodes = []\n",
    "        for i, node in enumerate(sample['program']):\n",
    "            _fn_toks = node['function'].split('_')\n",
    "            if len(_fn_toks) > 1 and _fn_toks[1] in concept_list:\n",
    "                _fn, _concept = _fn_toks\n",
    "            else:\n",
    "                _fn = '_'.join(_fn_toks)\n",
    "                _concept = None\n",
    "            _value = None if not node['value_inputs'] else node['value_inputs'][0]\n",
    "            _nodes.append(ProgramNode(step=i,\n",
    "                                     function=_fn,\n",
    "                                     inputs=node['inputs'],\n",
    "                                     concept_input=_concept,\n",
    "                                     value_input=_value,\n",
    "            ))\n",
    "        programs.append(_nodes)\n",
    "    return programs\n",
    "\n",
    "\n",
    "# do for training set\n",
    "train_progs = formalize_program_annots(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './checkpoints/clevr_programs/CLEVR_train_programs.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6c3f53a47b72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Alternatively, if you already have the checkpoint load from here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_progs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./checkpoints/clevr_programs/CLEVR_train_programs.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_progs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mProgramNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_progs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './checkpoints/clevr_programs/CLEVR_train_programs.json'"
     ]
    }
   ],
   "source": [
    "# Alternatively, if you already have the checkpoint load from here\n",
    "train_progs = json.load(open('./checkpoints/clevr_programs/train.json'))\n",
    "train_progs = [[ProgramNode(**node) for node in p] for p in train_progs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some processed program annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_progs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b592dc764272>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_progs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_progs' is not defined"
     ]
    }
   ],
   "source": [
    "pprint(train_progs[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for val and test datasets as well and save them under ``checkpoints`` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(root, \"checkpoints/clevr/trainval/programs\")\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "with open(os.path.join(save_dir, 'train.json'), 'w') as f:\n",
    "    json.dump([[p.__dict__ for p in ps] for ps in train_progs], f)\n",
    "    \n",
    "_ds = json.load(open(val_questions_path))['questions']\n",
    "_progs = formalize_program_annots(_ds)\n",
    "with open(os.path.join(save_dir, 'val.json'), 'w') as f:\n",
    "    json.dump([[p.__dict__ for p in ps] for ps in _progs], f)\n",
    "\n",
    "# TEST set has no program annots :P\n",
    "# _ds = json.load(open(test_questions_path))['questions']\n",
    "# _progs = formalize_program_annots(_ds)\n",
    "# with open(os.path.join(save_dir, 'CLEVR_test_programs.json'), 'w') as f:\n",
    "#     json.dump([[p.__dict__ for p in ps] for ps in _progs], f)\n",
    "\n",
    "del _ds, _progs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We develop some tools to generate the program vocabulary and tokenize the datasets. Note that we convert all programs to chains and reverse them as the final annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from program_tokenizer import ProgramTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v.0) Domain-Specific Version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets tokenize the training programs with all domain information (*concept-aware, vocab-aware*) and spectate the resulting vocabulary. It looks exactly like the ``all_primitives`` set we extracted before, with the adittion of the special tags for decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'<END>': 2,\n",
      " '<PAD>': 0,\n",
      " '<START>': 1,\n",
      " '<UNK>': 3,\n",
      " 'count': 4,\n",
      " 'equal_integer': 5,\n",
      " 'equal{color}': 6,\n",
      " 'equal{material}': 7,\n",
      " 'equal{shape}': 8,\n",
      " 'equal{size}': 9,\n",
      " 'exist': 10,\n",
      " 'filter{color}[blue]': 11,\n",
      " 'filter{color}[brown]': 12,\n",
      " 'filter{color}[cyan]': 13,\n",
      " 'filter{color}[gray]': 14,\n",
      " 'filter{color}[green]': 15,\n",
      " 'filter{color}[purple]': 16,\n",
      " 'filter{color}[red]': 17,\n",
      " 'filter{color}[yellow]': 18,\n",
      " 'filter{material}[metal]': 19,\n",
      " 'filter{material}[rubber]': 20,\n",
      " 'filter{shape}[cube]': 21,\n",
      " 'filter{shape}[cylinder]': 22,\n",
      " 'filter{shape}[sphere]': 23,\n",
      " 'filter{size}[large]': 24,\n",
      " 'filter{size}[small]': 25,\n",
      " 'greater_than': 26,\n",
      " 'intersect': 27,\n",
      " 'less_than': 28,\n",
      " 'query{color}': 29,\n",
      " 'query{material}': 30,\n",
      " 'query{shape}': 31,\n",
      " 'query{size}': 32,\n",
      " 'relate[behind]': 33,\n",
      " 'relate[front]': 34,\n",
      " 'relate[left]': 35,\n",
      " 'relate[right]': 36,\n",
      " 'same{color}': 37,\n",
      " 'same{material}': 38,\n",
      " 'same{shape}': 39,\n",
      " 'same{size}': 40,\n",
      " 'scene': 41,\n",
      " 'union': 42,\n",
      " 'unique': 43}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ProgramTokenizer(version=0)\n",
    "tokenizer.make_from_dataset(train_progs)\n",
    "#Vocab = json.load(open('checkpoints/clevr_programs/CLEVR_vocabularies.json'))['prog2id']\n",
    "#tokenizer.make_vocab(Vocab)\n",
    "\n",
    "Vocab = tokenizer.vocab\n",
    "pprint(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize some programs to see the structure of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0): scene(),\n",
      "  (1): filter{size}[large](0),\n",
      "  (2): filter{color}[green](1),\n",
      "  (3): count(2),\n",
      "  (4): scene(),\n",
      "  (5): filter{size}[large](4),\n",
      "  (6): filter{color}[purple](5),\n",
      "  (7): filter{material}[metal](6),\n",
      "  (8): filter{shape}[cube](7),\n",
      "  (9): count(8),\n",
      "  (10): greater_than(3,9)],\n",
      " [(0): scene(),\n",
      "  (1): filter{size}[small](0),\n",
      "  (2): filter{color}[cyan](1),\n",
      "  (3): filter{material}[rubber](2),\n",
      "  (4): unique(3),\n",
      "  (5): same{shape}(4),\n",
      "  (6): count(5)]]\n",
      "\n",
      "[['scene',\n",
      "  'filter{size}[large]',\n",
      "  'filter{color}[green]',\n",
      "  'count',\n",
      "  'scene',\n",
      "  'filter{size}[large]',\n",
      "  'filter{color}[purple]',\n",
      "  'filter{material}[metal]',\n",
      "  'filter{shape}[cube]',\n",
      "  'count',\n",
      "  'greater_than'],\n",
      " ['scene',\n",
      "  'filter{size}[small]',\n",
      "  'filter{color}[cyan]',\n",
      "  'filter{material}[rubber]',\n",
      "  'unique',\n",
      "  'same{shape}',\n",
      "  'count']]\n"
     ]
    }
   ],
   "source": [
    "example_programs = train_progs[:2]\n",
    "pprint(example_programs)\n",
    "print()\n",
    "\n",
    "pprint(tokenizer.convert_programs_to_tokens(example_programs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are identical to the program representation, with the exception of dropping dependency inputs. These will be figured out by the program executor, as the program is structured as a chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually encode-decode some program tokens to see the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program tokens:\n",
      "['scene', 'filter{size}[small]', 'filter{color}[yellow]', 'filter{material}[metal]', 'unique', 'same{shape}', 'exist']\n",
      "\n",
      "Encoded ids:\n",
      "tensor([ 1, 10, 39, 43, 19, 18, 25, 41,  2])\n",
      "\n",
      "Raw conversion to tokens:\n",
      "['<START>', 'exist', 'same{shape}', 'unique', 'filter{material}[metal]', 'filter{color}[yellow]', 'filter{size}[small]', 'scene', '<END>']\n",
      "\n",
      "Decoded tokens:\n",
      "['scene', 'filter{size}[small]', 'filter{color}[yellow]', 'filter{material}[metal]', 'unique', 'same{shape}', 'exist']\n"
     ]
    }
   ],
   "source": [
    "print('Example program tokens:')\n",
    "example_program = train_progs[1231]\n",
    "example_tokens = tokenizer.tokenize(example_program)\n",
    "print(example_tokens)\n",
    "print()\n",
    "\n",
    "print('Encoded ids:')\n",
    "encoded = tokenizer.encode(example_tokens)\n",
    "print(encoded)\n",
    "print()\n",
    "print('Raw conversion to tokens:')\n",
    "print(tokenizer.convert_ids_to_tokens([encoded.tolist()])[0])\n",
    "print()\n",
    "print('Decoded tokens:')\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done for many samples at the same time with the use of ``batch_`` prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = 32\n",
      "[11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n",
      "\n",
      "Encoded size = torch.Size([32, 19])\n",
      "\n",
      "32\n",
      "[11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "example_batch = tokenizer.convert_programs_to_tokens(train_progs[:32])\n",
    "print(f'Batch size = {len(example_batch)}')\n",
    "print([len(p) for p in example_batch])\n",
    "print()\n",
    "\n",
    "encoded = tokenizer.batch_encode(example_batch)\n",
    "print(f'Encoded size = {encoded.shape}')\n",
    "print()\n",
    "\n",
    "decoded = tokenizer.batch_decode(encoded)\n",
    "print(len(decoded))\n",
    "print([len(p) for p in decoded])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same directly from ``ProgramNode`` representation by using the ``_program`` suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program:\n",
      "[(0): scene(),\n",
      " (1): filter{size}[large](0),\n",
      " (2): filter{color}[gray](1),\n",
      " (3): filter{shape}[cylinder](2),\n",
      " (4): unique(3),\n",
      " (5): relate[right](4),\n",
      " (6): filter{size}[large](5),\n",
      " (7): filter{material}[metal](6),\n",
      " (8): unique(7),\n",
      " (9): relate[left](8),\n",
      " (10): filter{color}[brown](9),\n",
      " (11): filter{shape}[cube](10),\n",
      " (12): count(11)]\n",
      "Encoded ids:\n",
      "tensor([ 1,  4, 21, 12, 35, 43, 19, 24, 36, 43, 22, 14, 24, 41,  2])\n",
      "\n",
      "Raw conversion to programs:\n",
      "[(0): <START>(-1), (1): count(0), (2): filter{shape}[cube](1), (3): filter{color}[brown](2), (4): relate[left](3), (5): unique(4), (6): filter{material}[metal](5), (7): filter{size}[large](6), (8): relate[right](7), (9): unique(8), (10): filter{shape}[cylinder](9), (11): filter{color}[gray](10), (12): filter{size}[large](11), (13): scene(), (14): <END>(13)]\n",
      "\n",
      "Decoded programs:\n",
      "[(0): scene(), (1): filter{size}[large](0), (2): filter{color}[gray](1), (3): filter{shape}[cylinder](2), (4): unique(3), (5): relate[right](4), (6): filter{size}[large](5), (7): filter{material}[metal](6), (8): unique(7), (9): relate[left](8), (10): filter{color}[brown](9), (11): filter{shape}[cube](10), (12): count(11)]\n",
      "\n",
      "32 [11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n",
      "torch.Size([32, 19])\n",
      "32 [11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "print('Example program:')\n",
    "example_program = train_progs[1312]\n",
    "pprint(example_program)\n",
    "\n",
    "print('Encoded ids:')\n",
    "encoded = tokenizer.encode_program(example_program)\n",
    "print(encoded)\n",
    "print()\n",
    "print('Raw conversion to programs:')\n",
    "print(tokenizer.convert_ids_to_programs([encoded.tolist()])[0])\n",
    "print()\n",
    "print('Decoded programs:')\n",
    "decoded = tokenizer.decode_program(encoded)\n",
    "print(decoded)\n",
    "print()\n",
    "\n",
    "# and same for multiple sample adding batch prefix\n",
    "example_batch = train_progs[:32]\n",
    "print(len(example_batch), [len(p) for p in example_batch])\n",
    "encoded = tokenizer.batch_encode_program(example_batch)\n",
    "decoded = tokenizer.batch_decode_program(encoded)\n",
    "print(encoded.shape)\n",
    "print(len(decoded), [len(p) for p in decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the programs to token IDs and save the checkpoint datasets for both train and val set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoints(train_progs, tokenizer, save_dir, val_path):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    with open(os.path.join(save_dir, 'vocab.json'), 'w') as f:\n",
    "        json.dump(tokenizer.vocab, f)\n",
    "\n",
    "    train_prog_ids = tokenizer.batch_encode_program(train_progs)\n",
    "    np.save(os.path.join(save_dir,'train_ids.npy' ), train_prog_ids.numpy())\n",
    "\n",
    "    _ds = json.load(open(val_questions_path))['questions']\n",
    "    _progs = formalize_program_annots(_ds)\n",
    "    _progs = tokenizer.batch_encode_program(_progs)\n",
    "    np.save(os.path.join(save_dir,'val_ids.npy'), _progs.numpy())\n",
    "\n",
    "    del _ds, _progs\n",
    "\n",
    "    \n",
    "save_dir = os.path.join(root, \"checkpoints/clevr/trainval/programs/v0\")\n",
    "save_checkpoints(train_progs, tokenizer, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v.1) Vocab-agnostic Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same as before but use the special tokens ``[, ]`` to define specific concept values as arguments, untangled from primitives, in order to reach the *vocab-agnostic* version of the library that we mention above. For a learning system, the specific concept values will be chosen from the input query, therefore freeing the primitives library from specific vocabulary. This will hopefully enable generalization,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<END>': 2,\n",
      " '<PAD>': 0,\n",
      " '<START>': 1,\n",
      " '<UNK>': 3,\n",
      " '<[>': 6,\n",
      " '<]>': 7,\n",
      " 'count': 8,\n",
      " 'equal_integer': 9,\n",
      " 'equal{color}': 10,\n",
      " 'equal{material}': 11,\n",
      " 'equal{shape}': 12,\n",
      " 'equal{size}': 13,\n",
      " 'exist': 14,\n",
      " 'filter{color}': 15,\n",
      " 'filter{material}': 16,\n",
      " 'filter{shape}': 17,\n",
      " 'filter{size}': 18,\n",
      " 'greater_than': 19,\n",
      " 'intersect': 20,\n",
      " 'less_than': 21,\n",
      " 'query{color}': 22,\n",
      " 'query{material}': 23,\n",
      " 'query{shape}': 24,\n",
      " 'query{size}': 25,\n",
      " 'relate': 26,\n",
      " 'same{color}': 27,\n",
      " 'same{material}': 28,\n",
      " 'same{shape}': 29,\n",
      " 'same{size}': 30,\n",
      " 'scene': 31,\n",
      " 'union': 32,\n",
      " 'unique': 33}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ProgramTokenizer(version=1)\n",
    "tokenizer.make_from_dataset(train_progs)\n",
    "#Vocab = json.load(open('checkpoints/clevr_programs/CLEVR_vocabularies.json'))['prog2id']\n",
    "#tokenizer.make_vocab(Vocab)\n",
    "\n",
    "Vocab = tokenizer.vocab\n",
    "pprint(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the two extra special tokens, that signature the beginning and end of a concept value argument. Our primitives are now free of the domain vocabulary!\n",
    "\n",
    "Let's do the same as before to see the structure of tokens, encode-decode some programs and save train-val checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program:\n",
      "[(0): scene(), (1): filter{size}[small](0), (2): filter{color}[yellow](1), (3): filter{material}[metal](2), (4): unique(3), (5): same{shape}(4), (6): exist(5)]\n",
      "\n",
      "Example program tokens:\n",
      "['scene', 'filter{size}', '<[>', 'small', '<]>', 'filter{color}', '<[>', 'yellow', '<]>', 'filter{material}', '<[>', 'metal', '<]>', 'unique', 'same{shape}', 'exist']\n",
      "\n",
      "Encoded ids:\n",
      "tensor([ 1, 14, 29, 33,  7,  3,  6, 16,  7,  3,  6, 15,  7,  3,  6, 18, 31,  2])\n",
      "\n",
      "Raw conversion to tokens:\n",
      "['<START>', 'exist', 'same{shape}', 'unique', '<]>', '<UNK>', '<[>', 'filter{material}', '<]>', '<UNK>', '<[>', 'filter{color}', '<]>', '<UNK>', '<[>', 'filter{size}', 'scene', '<END>']\n",
      "\n",
      "Decoded tokens:\n",
      "['scene', 'filter{size}', '<[>', '<UNK>', '<]>', 'filter{color}', '<[>', '<UNK>', '<]>', 'filter{material}', '<[>', '<UNK>', '<]>', 'unique', 'same{shape}', 'exist']\n"
     ]
    }
   ],
   "source": [
    "print('Example program:')\n",
    "example_program = train_progs[1231]\n",
    "print(example_program)\n",
    "print()\n",
    "\n",
    "print('Example program tokens:')\n",
    "example_tokens = tokenizer._tokenize(example_program)\n",
    "print(example_tokens)\n",
    "print()\n",
    "\n",
    "print('Encoded ids:')\n",
    "encoded = tokenizer.encode(example_tokens)\n",
    "print(encoded)\n",
    "print()\n",
    "print('Raw conversion to tokens:')\n",
    "print(tokenizer.convert_ids_to_tokens([encoded.tolist()])[0])\n",
    "print()\n",
    "print('Decoded tokens:')\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this version is *vocab-agnostic* it doesn't have a token for each specific concept value. Instead, specific values are replaced by the unknown ``<UNK>`` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program:\n",
      "[(0): scene(), (1): filter{size}[large](0), (2): filter{color}[gray](1), (3): filter{shape}[cylinder](2), (4): unique(3), (5): relate[right](4), (6): filter{size}[large](5), (7): filter{material}[metal](6), (8): unique(7), (9): relate[left](8), (10): filter{color}[brown](9), (11): filter{shape}[cube](10), (12): count(11)]\n",
      "\n",
      "Example program tokens:\n",
      "['scene', 'filter{size}', '<[>', 'large', '<]>', 'filter{color}', '<[>', 'gray', '<]>', 'filter{shape}', '<[>', 'cylinder', '<]>', 'unique', 'relate', '<[>', 'right', '<]>', 'filter{size}', '<[>', 'large', '<]>', 'filter{material}', '<[>', 'metal', '<]>', 'unique', 'relate', '<[>', 'left', '<]>', 'filter{color}', '<[>', 'brown', '<]>', 'filter{shape}', '<[>', 'cube', '<]>', 'count']\n",
      "\n",
      "Encoded ids:\n",
      "tensor([ 1,  8,  7,  3,  6, 17,  7,  3,  6, 15,  7,  3,  6, 26, 33,  7,  3,  6,\n",
      "        16,  7,  3,  6, 18,  7,  3,  6, 26, 33,  7,  3,  6, 17,  7,  3,  6, 15,\n",
      "         7,  3,  6, 18, 31,  2])\n",
      "\n",
      "Raw conversion to programs:\n",
      "[(0): <END>(-1), (1): scene(), (2): filter{size}[<UNK>](1), (3): filter{color}[<UNK>](2), (4): filter{shape}[<UNK>](3), (5): unique(4), (6): relate[<UNK>](5), (7): filter{size}[<UNK>](6), (8): filter{material}[<UNK>](7), (9): unique(8), (10): relate[<UNK>](9), (11): filter{color}[<UNK>](10), (12): filter{shape}[<UNK>](11), (13): count(12), (14): <START>(13)]\n",
      "\n",
      "Decoded programs:\n",
      "[(0): scene(), (1): filter{size}[<UNK>](0), (2): filter{color}[<UNK>](1), (3): filter{shape}[<UNK>](2), (4): unique(3), (5): relate[<UNK>](4), (6): filter{size}[<UNK>](5), (7): filter{material}[<UNK>](6), (8): unique(7), (9): relate[<UNK>](8), (10): filter{color}[<UNK>](9), (11): filter{shape}[<UNK>](10), (12): count(11)]\n",
      "\n",
      "32 [11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n",
      "torch.Size([32, 55])\n",
      "32 [11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "# Directly from ProgramNode representations\n",
    "\n",
    "print('Example program:')\n",
    "example_program = train_progs[1312]\n",
    "print(example_program)\n",
    "print()\n",
    "\n",
    "print('Example program tokens:')\n",
    "example_tokens = tokenizer._tokenize(example_program)\n",
    "print(example_tokens)\n",
    "print()\n",
    "\n",
    "print('Encoded ids:')\n",
    "encoded = tokenizer.encode_program(example_program)\n",
    "print(encoded)\n",
    "print()\n",
    "\n",
    "print('Raw conversion to programs:')\n",
    "print(tokenizer.convert_ids_to_programs([encoded.tolist()])[0])\n",
    "print()\n",
    "\n",
    "print('Decoded programs:')\n",
    "decoded = tokenizer.decode_program(encoded)\n",
    "print(decoded)\n",
    "print()\n",
    "\n",
    "# and same for multiple sample adding batch prefix\n",
    "example_batch = train_progs[:32]\n",
    "print(len(example_batch), [len(p) for p in example_batch])\n",
    "encoded = tokenizer.batch_encode_program(example_batch)\n",
    "decoded = tokenizer.batch_decode_program(encoded)\n",
    "print(encoded.shape)\n",
    "print(len(decoded), [len(p) for p in decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(root, \"checkpoints/clevr/trainval/programs/v1\")\n",
    "save_checkpoints(train_progs, tokenizer, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v.2) Concept-agnostic Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same as before but use the special tokens ``{, }, [, ]`` to define both concepts and concept values as arguments, untangled from primitives, in order to reach the *concept-agnostic* version of the library that we mention above. For a learning system, the specific concepts and concept values will be chosen from the input query and the domain specification, therefore freeing the primitives library from specific vocabulary, while enabling it to generalize to novel combinations of primitives and concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<END>': 2,\n",
      " '<PAD>': 0,\n",
      " '<START>': 1,\n",
      " '<UNK>': 3,\n",
      " '<[>': 10,\n",
      " '<]>': 11,\n",
      " '<{>': 12,\n",
      " '<}>': 13,\n",
      " 'color': 14,\n",
      " 'count': 15,\n",
      " 'equal': 16,\n",
      " 'equal_integer': 17,\n",
      " 'exist': 18,\n",
      " 'filter': 19,\n",
      " 'greater_than': 20,\n",
      " 'intersect': 21,\n",
      " 'less_than': 22,\n",
      " 'material': 23,\n",
      " 'query': 24,\n",
      " 'relate': 25,\n",
      " 'same': 26,\n",
      " 'scene': 27,\n",
      " 'shape': 28,\n",
      " 'size': 29,\n",
      " 'union': 30,\n",
      " 'unique': 31}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ProgramTokenizer(version=2)\n",
    "tokenizer.make_from_dataset(train_progs)\n",
    "#Vocab = json.load(open('checkpoints/clevr_programs/CLEVR_vocabularies.json'))['prog2id']\n",
    "#tokenizer.make_vocab(Vocab)\n",
    "\n",
    "Vocab = tokenizer.vocab\n",
    "pprint(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's as before, with the two extra special tokens ```{, }``` to denote beginning and end of concept arguments. The arguments themselves are also part of the program vocabulary.\n",
    "\n",
    "Let's inspect them as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program:\n",
      "[(0): scene(), (1): filter{size}[small](0), (2): filter{color}[yellow](1), (3): filter{material}[metal](2), (4): unique(3), (5): same{shape}(4), (6): exist(5)]\n",
      "\n",
      "Example program tokens:\n",
      "['scene', 'filter', '<{>', 'size', '<}>', '<[>', 'small', '<]>', 'filter', '<{>', 'color', '<}>', '<[>', 'yellow', '<]>', 'filter', '<{>', 'material', '<}>', '<[>', 'metal', '<]>', 'unique', 'same', '<{>', 'shape', '<}>', 'exist']\n",
      "\n",
      "Encoded ids:\n",
      "tensor([ 1, 18, 13, 28, 12, 26, 31, 11,  3, 10, 13, 23, 12, 19, 11,  3, 10, 13,\n",
      "        14, 12, 19, 11,  3, 10, 13, 29, 12, 19, 27,  2])\n",
      "\n",
      "Raw conversion to tokens:\n",
      "['<START>', 'exist', '<}>', 'shape', '<{>', 'same', 'unique', '<]>', '<UNK>', '<[>', '<}>', 'material', '<{>', 'filter', '<]>', '<UNK>', '<[>', '<}>', 'color', '<{>', 'filter', '<]>', '<UNK>', '<[>', '<}>', 'size', '<{>', 'filter', 'scene', '<END>']\n",
      "\n",
      "Decoded tokens:\n",
      "['scene', 'filter', '<{>', 'size', '<}>', '<[>', '<UNK>', '<]>', 'filter', '<{>', 'color', '<}>', '<[>', '<UNK>', '<]>', 'filter', '<{>', 'material', '<}>', '<[>', '<UNK>', '<]>', 'unique', 'same', '<{>', 'shape', '<}>', 'exist']\n"
     ]
    }
   ],
   "source": [
    "print('Example program:')\n",
    "example_program = train_progs[1231]\n",
    "print(example_program)\n",
    "print()\n",
    "\n",
    "print('Example program tokens:')\n",
    "example_tokens = tokenizer._tokenize(example_program)\n",
    "print(example_tokens)\n",
    "print()\n",
    "\n",
    "print('Encoded ids:')\n",
    "encoded = tokenizer.encode(example_tokens)\n",
    "print(encoded)\n",
    "print()\n",
    "print('Raw conversion to tokens:')\n",
    "print(tokenizer.convert_ids_to_tokens([encoded.tolist()])[0])\n",
    "print()\n",
    "print('Decoded tokens:')\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program:\n",
      "[(0): scene(), (1): filter{size}[large](0), (2): filter{color}[gray](1), (3): filter{shape}[cylinder](2), (4): unique(3), (5): relate[right](4), (6): filter{size}[large](5), (7): filter{material}[metal](6), (8): unique(7), (9): relate[left](8), (10): filter{color}[brown](9), (11): filter{shape}[cube](10), (12): count(11)]\n",
      "\n",
      "Example program tokens:\n",
      "['scene', 'filter', '<{>', 'size', '<}>', '<[>', 'large', '<]>', 'filter', '<{>', 'color', '<}>', '<[>', 'gray', '<]>', 'filter', '<{>', 'shape', '<}>', '<[>', 'cylinder', '<]>', 'unique', 'relate', '<[>', 'right', '<]>', 'filter', '<{>', 'size', '<}>', '<[>', 'large', '<]>', 'filter', '<{>', 'material', '<}>', '<[>', 'metal', '<]>', 'unique', 'relate', '<[>', 'left', '<]>', 'filter', '<{>', 'color', '<}>', '<[>', 'brown', '<]>', 'filter', '<{>', 'shape', '<}>', '<[>', 'cube', '<]>', 'count']\n",
      "\n",
      "Encoded ids:\n",
      "tensor([ 1, 15, 11,  3, 10, 13, 28, 12, 19, 11,  3, 10, 13, 14, 12, 19, 11,  3,\n",
      "        10, 25, 31, 11,  3, 10, 13, 23, 12, 19, 11,  3, 10, 13, 29, 12, 19, 11,\n",
      "         3, 10, 25, 31, 11,  3, 10, 13, 28, 12, 19, 11,  3, 10, 13, 14, 12, 19,\n",
      "        11,  3, 10, 13, 29, 12, 19, 27,  2])\n",
      "\n",
      "Raw conversion to programs:\n",
      "[(0): <END>(-1), (1): scene(), (2): filter{size}[<UNK>](1), (3): filter{color}[<UNK>](2), (4): filter{shape}[<UNK>](3), (5): unique(4), (6): relate[<UNK>](5), (7): filter{size}[<UNK>](6), (8): filter{material}[<UNK>](7), (9): unique(8), (10): relate[<UNK>](9), (11): filter{color}[<UNK>](10), (12): filter{shape}[<UNK>](11), (13): count(12), (14): <START>(13)]\n",
      "\n",
      "Decoded programs:\n",
      "[(0): scene(), (1): filter{size}[<UNK>](0), (2): filter{color}[<UNK>](1), (3): filter{shape}[<UNK>](2), (4): unique(3), (5): relate[<UNK>](4), (6): filter{size}[<UNK>](5), (7): filter{material}[<UNK>](6), (8): unique(7), (9): relate[<UNK>](8), (10): filter{color}[<UNK>](9), (11): filter{shape}[<UNK>](10), (12): count(11)]\n",
      "\n",
      "32 [11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n",
      "torch.Size([32, 82])\n",
      "32 [11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "# Directly from ProgramNode representations\n",
    "\n",
    "print('Example program:')\n",
    "example_program = train_progs[1312]\n",
    "print(example_program)\n",
    "print()\n",
    "\n",
    "print('Example program tokens:')\n",
    "example_tokens = tokenizer._tokenize(example_program)\n",
    "print(example_tokens)\n",
    "print()\n",
    "\n",
    "print('Encoded ids:')\n",
    "encoded = tokenizer.encode_program(example_program)\n",
    "print(encoded)\n",
    "print()\n",
    "\n",
    "print('Raw conversion to programs:')\n",
    "print(tokenizer.convert_ids_to_programs([encoded.tolist()])[0])\n",
    "print()\n",
    "\n",
    "print('Decoded programs:')\n",
    "decoded = tokenizer.decode_program(encoded)\n",
    "print(decoded)\n",
    "print()\n",
    "\n",
    "# and same for multiple sample adding batch prefix\n",
    "example_batch = train_progs[:32]\n",
    "print(len(example_batch), [len(p) for p in example_batch])\n",
    "encoded = tokenizer.batch_encode_program(example_batch)\n",
    "decoded = tokenizer.batch_decode_program(encoded)\n",
    "print(encoded.shape)\n",
    "print(len(decoded), [len(p) for p in decoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(root, \"checkpoints/clevr/programs/v2\")\n",
    "save_checkpoints(train_progs, tokenizer, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating generalization splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides evaluating in the validation split of CLEVR dataset, we wish to asses the generalization performance of a semantic parser that  maps language to programs. To that end, we will create 3 generalization test splits:\n",
    "\n",
    "  - **Novel Combinations**: Test in unseen combinations of attributes, e.g. train in red cubes and blue spheres and evaluate for blue cubes and red spheres.\n",
    "  - **Novel Vocabulary**: Test in unseen concept values, e.g. train in cubes and cylinders and evaluate for spheres.\n",
    "  - **Novel Tasks**: Test in unseen tasks, i.e. unseen combinations of primitives and concepts, e.g. train for filtering shape and querying for color, and evaluate for filtering color and querying for shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-A) Novel Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CLEVR dataset actually comes with a pre-generated dataset for this generalization test, it is caled CoGEN-T, and you can download it together with the original dataset. In this setup, there are two conditions: A and B. In condition A there are gray, blue, brown, or yellow cubes, red, green, purple, or cyan cylinders and any color spheres, while in condition B the colors between cubes and cylinders are reversed. The experiment involves training on condition A and then evaluating in condition B with no further training (*zero-shot*) or with fine-tuning in few examples (*few-shot*). We will use the zero-shot setup in order to evaluate the combinatorial generalization abilities of learning systems. We will use the training and validation sets of condition A to train the model and validation set of condition B to test it.\n",
    "\n",
    "All we have to do then is repeat the above steps for the downloaded CoGEN-T dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(root, \"checkpoints/clevr/testA/programs\")\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "_ds = json.load(open(gen_trainA_questions_path))['questions']\n",
    "_progs = formalize_program_annots(_ds)\n",
    "with open(os.path.join(save_dir, 'train.json'), 'w') as f:\n",
    "    json.dump([[p.__dict__ for p in ps] for ps in train_progs], f)\n",
    "    \n",
    "_ds = json.load(open(gen_valA_questions_path))['questions']\n",
    "_progs = formalize_program_annots(_ds)\n",
    "with open(os.path.join(save_dir, 'val.json'), 'w') as f:\n",
    "    json.dump([[p.__dict__ for p in ps] for ps in _progs], f)\n",
    "\n",
    "_ds = json.load(open(gen_valB_questions_path))['questions']\n",
    "_progs = formalize_program_annots(_ds)\n",
    "with open(os.path.join(save_dir, 'test.json'), 'w') as f:\n",
    "    json.dump([[p.__dict__ for p in ps] for ps in _progs], f)\n",
    "\n",
    "del _ds, _progs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save tokenized checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9cf461cc7b29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0msave_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-9cf461cc7b29>\u001b[0m in \u001b[0;36msave_checkpoints\u001b[0;34m(save_dir, path, name)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProgramTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_progs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0m_save_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_progs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'v{version}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9cf461cc7b29>\u001b[0m in \u001b[0;36m_save_checkpoints\u001b[0;34m(progs, tokenizer, save_dir, name)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{name}_ids.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprog_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_program\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mprog_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m#np.save(os.path.join(save_dir, name), prog_ids.numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-9cf461cc7b29>\u001b[0m in \u001b[0;36munpad\u001b[0;34m(X, eos_token_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0meos_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0meos_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meos_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def unpad(X, eos_token_id=2):\n",
    "    eos_mask = (torch.where(X == eos_token_id)[1] + 1).tolist()\n",
    "    token_ids = [toks[:idx] for toks, idx in zip(X.tolist(), eos_mask)]\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "def save_checkpoints(save_dir, path, name):\n",
    "    def _save_checkpoints(progs, tokenizer, save_dir, name):\n",
    "        if not os.path.isdir(save_dir):\n",
    "            os.mkdir(save_dir)\n",
    "\n",
    "        with open(os.path.join(save_dir, 'vocab.json'), 'w') as f:\n",
    "            json.dump(tokenizer.vocab, f)\n",
    "\n",
    "        name = f'{name}_ids.json'\n",
    "        prog_ids = tokenizer.batch_encode_program(progs)\n",
    "        prog_ids = unpad(prog_ids)\n",
    "        #np.save(os.path.join(save_dir, name), prog_ids.numpy())\n",
    "        with open(os.path.join(save_dir, name), 'w') as g:\n",
    "            json.dump(prog_ids, g)\n",
    "            \n",
    "    _ds = json.load(open(path))['questions']\n",
    "    _progs = formalize_program_annots(_ds)\n",
    "    \n",
    "    for version in [0, 1, 2]:\n",
    "        tokenizer = ProgramTokenizer(version=version)\n",
    "        _ = tokenizer.make_from_dataset(_progs)\n",
    "        _save_checkpoints(_progs, tokenizer, os.path.join(save_dir, f'v{version}'), name=name)\n",
    "        \n",
    "    \n",
    "save_dir = os.path.join(root, \"checkpoints/clevr/testA/programs\")\n",
    "paths = [gen_trainA_questions_path, gen_valA_questions_path, gen_valB_questions_path]\n",
    "names = ['train', 'val', 'test']\n",
    "for p, n in zip(paths, names):\n",
    "    save_checkpoints(save_dir, p, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-B) Novel Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-C) Novel Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing CLEVR programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
