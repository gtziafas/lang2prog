{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/p300488/lang2prog\n"
     ]
    }
   ],
   "source": [
    " cd /data/p300488/lang2prog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring CLEVR questions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "clevr_path = '/data/p300488/datasets/clevr/CLEVR_v1.0'\n",
    "train_questions_path = os.path.join(clevr_path, 'questions/CLEVR_train_questions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read GQA questions dataset. Size of training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699989\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "ds = json.load(open(train_questions_path))['questions']\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the structure of a sample, it contains a program annotation for the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'yes',\n",
      " 'image_filename': 'CLEVR_train_000000.png',\n",
      " 'image_index': 0,\n",
      " 'program': [{'function': 'scene', 'inputs': [], 'value_inputs': []},\n",
      "             {'function': 'filter_size',\n",
      "              'inputs': [0],\n",
      "              'value_inputs': ['large']},\n",
      "             {'function': 'filter_color',\n",
      "              'inputs': [1],\n",
      "              'value_inputs': ['green']},\n",
      "             {'function': 'count', 'inputs': [2], 'value_inputs': []},\n",
      "             {'function': 'scene', 'inputs': [], 'value_inputs': []},\n",
      "             {'function': 'filter_size',\n",
      "              'inputs': [4],\n",
      "              'value_inputs': ['large']},\n",
      "             {'function': 'filter_color',\n",
      "              'inputs': [5],\n",
      "              'value_inputs': ['purple']},\n",
      "             {'function': 'filter_material',\n",
      "              'inputs': [6],\n",
      "              'value_inputs': ['metal']},\n",
      "             {'function': 'filter_shape',\n",
      "              'inputs': [7],\n",
      "              'value_inputs': ['cube']},\n",
      "             {'function': 'count', 'inputs': [8], 'value_inputs': []},\n",
      "             {'function': 'greater_than',\n",
      "              'inputs': [3, 9],\n",
      "              'value_inputs': []}],\n",
      " 'question': 'Are there more big green things than large purple shiny cubes?',\n",
      " 'question_family_index': 2,\n",
      " 'question_index': 0,\n",
      " 'split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'see all the different reasoning primitives and their related concept values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count',\n",
      " 'equal_color',\n",
      " 'equal_integer',\n",
      " 'equal_material',\n",
      " 'equal_shape',\n",
      " 'equal_size',\n",
      " 'exist',\n",
      " 'filter_color[blue]',\n",
      " 'filter_color[brown]',\n",
      " 'filter_color[cyan]',\n",
      " 'filter_color[gray]',\n",
      " 'filter_color[green]',\n",
      " 'filter_color[purple]',\n",
      " 'filter_color[red]',\n",
      " 'filter_color[yellow]',\n",
      " 'filter_material[metal]',\n",
      " 'filter_material[rubber]',\n",
      " 'filter_shape[cube]',\n",
      " 'filter_shape[cylinder]',\n",
      " 'filter_shape[sphere]',\n",
      " 'filter_size[large]',\n",
      " 'filter_size[small]',\n",
      " 'greater_than',\n",
      " 'intersect',\n",
      " 'less_than',\n",
      " 'query_color',\n",
      " 'query_material',\n",
      " 'query_shape',\n",
      " 'query_size',\n",
      " 'relate[behind]',\n",
      " 'relate[front]',\n",
      " 'relate[left]',\n",
      " 'relate[right]',\n",
      " 'same_color',\n",
      " 'same_material',\n",
      " 'same_shape',\n",
      " 'same_size',\n",
      " 'scene',\n",
      " 'union',\n",
      " 'unique'}\n"
     ]
    }
   ],
   "source": [
    "all_primitives = set()\n",
    "for sample in ds:\n",
    "    for node in sample['program']:\n",
    "        _fn = node['function']\n",
    "        _side_input =  '[' + node['value_inputs'][0] + ']' if node['value_inputs'] else ''\n",
    "        all_primitives.add(_fn + _side_input)\n",
    "\n",
    "pprint(all_primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this formalism, the primitives are both concept-aware (``filter_color, filter_size`` etc.), as well as vocabulary-aware (``filter_color[red], filter_color[blue]``, etc ). Let's create a version which decouples specific concept values from the primitives (*vocabulary-agnostic*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count',\n",
      " 'equal_color',\n",
      " 'equal_integer',\n",
      " 'equal_material',\n",
      " 'equal_shape',\n",
      " 'equal_size',\n",
      " 'exist',\n",
      " 'filter_color',\n",
      " 'filter_material',\n",
      " 'filter_shape',\n",
      " 'filter_size',\n",
      " 'greater_than',\n",
      " 'intersect',\n",
      " 'less_than',\n",
      " 'query_color',\n",
      " 'query_material',\n",
      " 'query_shape',\n",
      " 'query_size',\n",
      " 'relate',\n",
      " 'same_color',\n",
      " 'same_material',\n",
      " 'same_shape',\n",
      " 'same_size',\n",
      " 'scene',\n",
      " 'union',\n",
      " 'unique'}\n"
     ]
    }
   ],
   "source": [
    "vocab_agnostic_primitives = set()\n",
    "concept_agnostic_primitives = set()\n",
    "for fn in all_primitives:\n",
    "    f = fn.split('[')[0]\n",
    "    vocab_agnostic_primitives.add(f)\n",
    "    if len(f.split('_')) > 1:\n",
    "        f = f.split('_')[0] if f.split('_')[1] not in ['than', 'integer'] else f\n",
    "    concept_agnostic_primitives.add(f)\n",
    "\n",
    "pprint(vocab_agnostic_primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the most general formalism, without concept-awareness (*concept_agnostic*)\\:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count',\n",
      " 'equal',\n",
      " 'equal_integer',\n",
      " 'exist',\n",
      " 'filter',\n",
      " 'greater_than',\n",
      " 'intersect',\n",
      " 'less_than',\n",
      " 'query',\n",
      " 'relate',\n",
      " 'same',\n",
      " 'scene',\n",
      " 'union',\n",
      " 'unique'}\n"
     ]
    }
   ],
   "source": [
    "pprint(concept_agnostic_primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give some context on different primitive types:\n",
    "\n",
    "   - **Operational** : ``scene``: Initializes a set of objects given RGB image, ``unique``: {n} -> n\n",
    "   \n",
    "   - **Logical**: ``union/intersection``: union / intersection of two sets (outputs of two reasoning branches),\n",
    "   \n",
    "   - **Enumeration**: ``exist``: is a set non-empty?, ``count``: size of set, ``less_than/greater_than/equal_integer``: compares two integers\n",
    "   \n",
    "   - **Visual**: ``filter``: isolate object set based on attribute value, ``query``: ask for an attribute value, ``same``: object set which has same attribute value as given, ``equal``: whether two objects have equal attribute value\n",
    "   \n",
    "   - **Spatial**: ``relate``: object set which has certain spatial relation to given object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.6.0",
   "language": "python",
   "name": "sys_python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
