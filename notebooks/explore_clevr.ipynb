{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/p300488/lang2prog\n"
     ]
    }
   ],
   "source": [
    " cd /data/p300488/lang2prog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring CLEVR questions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root = '/data/p300488/lang2prog'\n",
    "clevr_path = '/data/p300488/datasets/clevr/CLEVR_v1.0'\n",
    "train_questions_path = os.path.join(clevr_path, 'questions/CLEVR_train_questions.json')\n",
    "val_questions_path = os.path.join(clevr_path, 'questions/CLEVR_val_questions.json')\n",
    "test_questions_path = os.path.join(clevr_path, 'questions/CLEVR_test_questions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CLEVR questions dataset. Size of training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "699989\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "ds = json.load(open(train_questions_path))['questions']\n",
    "print(len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the structure of a sample, it contains a program annotation for the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'yes',\n",
      " 'image_filename': 'CLEVR_train_000000.png',\n",
      " 'image_index': 0,\n",
      " 'program': [{'function': 'scene', 'inputs': [], 'value_inputs': []},\n",
      "             {'function': 'filter_size',\n",
      "              'inputs': [0],\n",
      "              'value_inputs': ['large']},\n",
      "             {'function': 'filter_color',\n",
      "              'inputs': [1],\n",
      "              'value_inputs': ['green']},\n",
      "             {'function': 'count', 'inputs': [2], 'value_inputs': []},\n",
      "             {'function': 'scene', 'inputs': [], 'value_inputs': []},\n",
      "             {'function': 'filter_size',\n",
      "              'inputs': [4],\n",
      "              'value_inputs': ['large']},\n",
      "             {'function': 'filter_color',\n",
      "              'inputs': [5],\n",
      "              'value_inputs': ['purple']},\n",
      "             {'function': 'filter_material',\n",
      "              'inputs': [6],\n",
      "              'value_inputs': ['metal']},\n",
      "             {'function': 'filter_shape',\n",
      "              'inputs': [7],\n",
      "              'value_inputs': ['cube']},\n",
      "             {'function': 'count', 'inputs': [8], 'value_inputs': []},\n",
      "             {'function': 'greater_than',\n",
      "              'inputs': [3, 9],\n",
      "              'value_inputs': []}],\n",
      " 'question': 'Are there more big green things than large purple shiny cubes?',\n",
      " 'question_family_index': 2,\n",
      " 'question_index': 0,\n",
      " 'split': 'train'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let'see all the different reasoning primitives and their related concept values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count',\n",
      " 'equal_color',\n",
      " 'equal_integer',\n",
      " 'equal_material',\n",
      " 'equal_shape',\n",
      " 'equal_size',\n",
      " 'exist',\n",
      " 'filter_color[blue]',\n",
      " 'filter_color[brown]',\n",
      " 'filter_color[cyan]',\n",
      " 'filter_color[gray]',\n",
      " 'filter_color[green]',\n",
      " 'filter_color[purple]',\n",
      " 'filter_color[red]',\n",
      " 'filter_color[yellow]',\n",
      " 'filter_material[metal]',\n",
      " 'filter_material[rubber]',\n",
      " 'filter_shape[cube]',\n",
      " 'filter_shape[cylinder]',\n",
      " 'filter_shape[sphere]',\n",
      " 'filter_size[large]',\n",
      " 'filter_size[small]',\n",
      " 'greater_than',\n",
      " 'intersect',\n",
      " 'less_than',\n",
      " 'query_color',\n",
      " 'query_material',\n",
      " 'query_shape',\n",
      " 'query_size',\n",
      " 'relate[behind]',\n",
      " 'relate[front]',\n",
      " 'relate[left]',\n",
      " 'relate[right]',\n",
      " 'same_color',\n",
      " 'same_material',\n",
      " 'same_shape',\n",
      " 'same_size',\n",
      " 'scene',\n",
      " 'union',\n",
      " 'unique'}\n"
     ]
    }
   ],
   "source": [
    "all_primitives = set()\n",
    "for sample in ds:\n",
    "    for node in sample['program']:\n",
    "        _fn = node['function']\n",
    "        _side_input =  '[' + node['value_inputs'][0] + ']' if node['value_inputs'] else ''\n",
    "        all_primitives.add(_fn + _side_input)\n",
    "\n",
    "pprint(all_primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this formalism, the primitives are both concept-aware (``filter_color, filter_size`` etc.), as well as vocabulary-aware (``filter_color[red], filter_color[blue]``, etc ). Let's create a version which decouples specific concept values from the primitives (*vocabulary-agnostic*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count',\n",
      " 'equal_color',\n",
      " 'equal_integer',\n",
      " 'equal_material',\n",
      " 'equal_shape',\n",
      " 'equal_size',\n",
      " 'exist',\n",
      " 'filter_color',\n",
      " 'filter_material',\n",
      " 'filter_shape',\n",
      " 'filter_size',\n",
      " 'greater_than',\n",
      " 'intersect',\n",
      " 'less_than',\n",
      " 'query_color',\n",
      " 'query_material',\n",
      " 'query_shape',\n",
      " 'query_size',\n",
      " 'relate',\n",
      " 'same_color',\n",
      " 'same_material',\n",
      " 'same_shape',\n",
      " 'same_size',\n",
      " 'scene',\n",
      " 'union',\n",
      " 'unique'}\n"
     ]
    }
   ],
   "source": [
    "vocab_agnostic_primitives = set()\n",
    "concept_agnostic_primitives = set()\n",
    "for fn in all_primitives:\n",
    "    f = fn.split('[')[0]\n",
    "    vocab_agnostic_primitives.add(f)\n",
    "    if len(f.split('_')) > 1:\n",
    "        f = f.split('_')[0] if f.split('_')[1] not in ['than', 'integer'] else f\n",
    "    concept_agnostic_primitives.add(f)\n",
    "\n",
    "pprint(vocab_agnostic_primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the most general formalism, without concept-awareness (*concept_agnostic*)\\:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count',\n",
      " 'equal',\n",
      " 'equal_integer',\n",
      " 'exist',\n",
      " 'filter',\n",
      " 'greater_than',\n",
      " 'intersect',\n",
      " 'less_than',\n",
      " 'query',\n",
      " 'relate',\n",
      " 'same',\n",
      " 'scene',\n",
      " 'union',\n",
      " 'unique'}\n"
     ]
    }
   ],
   "source": [
    "pprint(concept_agnostic_primitives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give some context on different primitive types:\n",
    "\n",
    "   - **Operational** : ``scene``: Initializes a set of objects given RGB image, ``unique``: {n} -> n\n",
    "   \n",
    "   - **Logical**: ``union/intersection``: union / intersection of two sets (outputs of two reasoning branches),\n",
    "   \n",
    "   - **Enumeration**: ``exist``: is a set non-empty?, ``count``: size of set, ``less_than/greater_than/equal_integer``: compares two integers\n",
    "   \n",
    "   - **Visual**: ``filter``: isolate object set based on attribute value, ``query``: ask for an attribute value, ``same``: object set which has same attribute value as given, ``equal``: whether two objects have equal attribute value\n",
    "   \n",
    "   - **Spatial**: ``relate``: object set which has certain spatial relation to given object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Language-to-Program datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the program annotations to a universal format, which also works for GQA dataset. The format breaks down each reasoning primitive as a ``ProgramNode`` object, consisting of a function call (*concept_agnostic*, e.g. ``filter``), a ``concept_input`` field (e.g. ``color``), a ``value_input`` field (e.g. ``[blue]``) and an ``inputs`` field, denoting the index of the reasoning steps whose output is input for the current step. \n",
    "\n",
    "This representation allows us to restructure the program annotations in whatever fashion (*concept_agnostic*, *vocab-agnostic* etc.) we want for different learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typings import *\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEVR_CONCEPTS = [\"color\", \"material\", \"size\", \"shape\"]\n",
    "\n",
    "def formalize_program_annots(ds, concept_list=CLEVR_CONCEPTS):\n",
    "    programs = []\n",
    "    for sample in ds:\n",
    "        _nodes = []\n",
    "        for i, node in enumerate(sample['program']):\n",
    "            _fn_toks = node['function'].split('_')\n",
    "            if len(_fn_toks) > 1 and _fn_toks[1] in concept_list:\n",
    "                _fn, _concept = _fn_toks\n",
    "            else:\n",
    "                _fn = '_'.join(_fn_toks)\n",
    "                _concept = None\n",
    "            _value = None if not node['value_inputs'] else node['value_inputs'][0]\n",
    "            _nodes.append(ProgramNode(step=i,\n",
    "                                     function=_fn,\n",
    "                                     inputs=node['inputs'],\n",
    "                                     concept_input=_concept,\n",
    "                                     value_input=_value,\n",
    "            ))\n",
    "        programs.append(_nodes)\n",
    "    return programs\n",
    "\n",
    "\n",
    "# do for training set\n",
    "train_progs = formalize_program_annots(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, if you already have the checkpoint load from here\n",
    "train_progs = json.load(open('./checkpoints/clevr_programs/CLEVR_train_programs.json'))\n",
    "train_progs = [[ProgramNode(**node) for node in p] for p in train_progs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some processed program annotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0): scene(),\n",
      "  (1): filter{size}[large](0),\n",
      "  (2): filter{color}[green](1),\n",
      "  (3): count(2),\n",
      "  (4): scene(),\n",
      "  (5): filter{size}[large](4),\n",
      "  (6): filter{color}[purple](5),\n",
      "  (7): filter{material}[metal](6),\n",
      "  (8): filter{shape}[cube](7),\n",
      "  (9): count(8),\n",
      "  (10): greater_than(3,9)],\n",
      " [(0): scene(),\n",
      "  (1): filter{size}[small](0),\n",
      "  (2): filter{color}[cyan](1),\n",
      "  (3): filter{material}[rubber](2),\n",
      "  (4): unique(3),\n",
      "  (5): same{shape}(4),\n",
      "  (6): count(5)],\n",
      " [(0): scene(),\n",
      "  (1): filter{size}[large](0),\n",
      "  (2): filter{shape}[sphere](1),\n",
      "  (3): unique(2),\n",
      "  (4): scene(),\n",
      "  (5): filter{size}[large](4),\n",
      "  (6): filter{material}[rubber](5),\n",
      "  (7): filter{shape}[cube](6),\n",
      "  (8): unique(7),\n",
      "  (9): query{color}(3),\n",
      "  (10): query{color}(8),\n",
      "  (11): equal{color}(9,10)],\n",
      " [(0): scene(),\n",
      "  (1): filter{size}[large](0),\n",
      "  (2): filter{color}[brown](1),\n",
      "  (3): filter{shape}[sphere](2),\n",
      "  (4): unique(3),\n",
      "  (5): relate[left](4),\n",
      "  (6): scene(),\n",
      "  (7): filter{color}[brown](6),\n",
      "  (8): filter{shape}[cylinder](7),\n",
      "  (9): unique(8),\n",
      "  (10): relate[right](9),\n",
      "  (11): intersect(5,10),\n",
      "  (12): filter{size}[large](11),\n",
      "  (13): unique(12),\n",
      "  (14): query{material}(13)],\n",
      " [(0): scene(),\n",
      "  (1): filter{color}[brown](0),\n",
      "  (2): filter{material}[metal](1),\n",
      "  (3): filter{shape}[sphere](2),\n",
      "  (4): unique(3),\n",
      "  (5): query{size}(4)]]\n"
     ]
    }
   ],
   "source": [
    "pprint(train_progs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat for val and test datasets as well and save them under ``checkpoints`` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(root, \"checkpoints/clevr_programs\")\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "with open(os.path.join(save_dir, 'CLEVR_train_programs.json'), 'w') as f:\n",
    "    json.dump([[p.__dict__ for p in ps] for ps in train_progs], f)\n",
    "    \n",
    "_ds = json.load(open(val_questions_path))['questions']\n",
    "_progs = formalize_program_annots(_ds)\n",
    "with open(os.path.join(save_dir, 'CLEVR_val_programs.json'), 'w') as f:\n",
    "    json.dump([[p.__dict__ for p in ps] for ps in _progs], f)\n",
    "\n",
    "# TEST set has no program annots :P\n",
    "# _ds = json.load(open(test_questions_path))['questions']\n",
    "# _progs = formalize_program_annots(_ds)\n",
    "# with open(os.path.join(save_dir, 'CLEVR_test_programs.json'), 'w') as f:\n",
    "#     json.dump([[p.__dict__ for p in ps] for ps in _progs], f)\n",
    "\n",
    "del _ds, _progs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We develop some tools to generate the program vocabulary and tokenize the datasets. Note that we convert all programs to chains and reverse them as the final annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from program_tokenizer import ProgramTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v.0) Domain-Specific Version "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets tokenize the training programs with all domain information (*concept-aware, vocab-aware*) and spectate the resulting vocabulary. It looks exactly like the ``all_primitives`` set we extracted before, with the adittion of the special tags for decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-86a1bc352d07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProgramTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_progs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Vocab = json.load(open('checkpoints/clevr_programs/CLEVR_vocabularies.json'))['prog2id']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#tokenizer.make_vocab(Vocab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/p300488/lang2prog/program_tokenizer.py\u001b[0m in \u001b[0;36mmake_from_dataset\u001b[0;34m(self, programs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprograms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mProgram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_programs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprograms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/p300488/lang2prog/program_tokenizer.py\u001b[0m in \u001b[0;36mpreprocess_programs\u001b[0;34m(self, programs)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_programs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprograms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mProgram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clevr_convert_to_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_programs_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprograms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/p300488/lang2prog/program_tokenizer.py\u001b[0m in \u001b[0;36mconvert_programs_to_tokens\u001b[0;34m(self, programs)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_programs_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprograms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mProgram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprograms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_tokens_to_programs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mProgram\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/p300488/lang2prog/program_tokenizer.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, program)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogram\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mProgram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_node_to_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogram\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mProgram\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/p300488/lang2prog/program_tokenizer.py\u001b[0m in \u001b[0;36m_convert_node_to_token\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_convert_node_to_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mProgramNode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0m_concept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcept_input\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"{\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcept_input\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_input\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"[\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_input\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m  \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_concept\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = ProgramTokenizer(version=0)\n",
    "tokenizer.make_from_dataset(train_progs)\n",
    "#Vocab = json.load(open('checkpoints/clevr_programs/CLEVR_vocabularies.json'))['prog2id']\n",
    "#tokenizer.make_vocab(Vocab)\n",
    "\n",
    "Vocab = tokenizer.vocab\n",
    "pprint(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize some programs to see the structure of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0): scene(),\n",
      "  (1): filter{size}[large](0),\n",
      "  (2): filter{color}[green](1),\n",
      "  (3): count(2),\n",
      "  (4): scene(),\n",
      "  (5): filter{size}[large](4),\n",
      "  (6): filter{color}[purple](5),\n",
      "  (7): filter{material}[metal](6),\n",
      "  (8): filter{shape}[cube](7),\n",
      "  (9): count(8),\n",
      "  (10): greater_than(3,9)],\n",
      " [(0): scene(),\n",
      "  (1): filter{size}[small](0),\n",
      "  (2): filter{color}[cyan](1),\n",
      "  (3): filter{material}[rubber](2),\n",
      "  (4): unique(3),\n",
      "  (5): same{shape}(4),\n",
      "  (6): count(5)]]\n",
      "\n",
      "[['scene',\n",
      "  'filter{size}[large]',\n",
      "  'filter{color}[green]',\n",
      "  'count',\n",
      "  'scene',\n",
      "  'filter{size}[large]',\n",
      "  'filter{color}[purple]',\n",
      "  'filter{material}[metal]',\n",
      "  'filter{shape}[cube]',\n",
      "  'count',\n",
      "  'greater_than'],\n",
      " ['scene',\n",
      "  'filter{size}[small]',\n",
      "  'filter{color}[cyan]',\n",
      "  'filter{material}[rubber]',\n",
      "  'unique',\n",
      "  'same{shape}',\n",
      "  'count']]\n"
     ]
    }
   ],
   "source": [
    "example_programs = train_progs[:2]\n",
    "pprint(example_programs)\n",
    "print()\n",
    "\n",
    "pprint(tokenizer.convert_programs_to_tokens(example_programs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are identical to the program representation, with the exception of dropping dependency inputs. These will be figured out by the program executor, as the program is structured as a chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's manually encode-decode some program tokens to see the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program tokens:\n",
      "['scene',\n",
      " 'filter{size}[small]',\n",
      " 'filter{color}[yellow]',\n",
      " 'filter{material}[metal]',\n",
      " 'unique',\n",
      " 'same{shape}',\n",
      " 'exist']\n",
      "\n",
      "Encoded ids:\n",
      "tensor([ 1, 10, 39, 43, 19, 18, 25, 41,  2])\n",
      "\n",
      "Raw conversion to tokens:\n",
      "['<START>',\n",
      " 'exist',\n",
      " 'same{shape}',\n",
      " 'unique',\n",
      " 'filter{material}[metal]',\n",
      " 'filter{color}[yellow]',\n",
      " 'filter{size}[small]',\n",
      " 'scene',\n",
      " '<END>']\n",
      "\n",
      "Decoded tokens:\n",
      "['scene',\n",
      " 'filter{size}[small]',\n",
      " 'filter{color}[yellow]',\n",
      " 'filter{material}[metal]',\n",
      " 'unique',\n",
      " 'same{shape}',\n",
      " 'exist']\n"
     ]
    }
   ],
   "source": [
    "print('Example program tokens:')\n",
    "example_program = train_progs[1231]\n",
    "example_tokens = tokenizer.tokenize(example_program)\n",
    "pprint(example_tokens)\n",
    "print()\n",
    "\n",
    "print('Encoded ids:')\n",
    "encoded = tokenizer.encode(example_tokens)\n",
    "print(encoded)\n",
    "print()\n",
    "print('Raw conversion to tokens:')\n",
    "pprint(tokenizer.convert_ids_to_tokens([encoded.tolist()])[0])\n",
    "print()\n",
    "print('Decoded tokens:')\n",
    "decoded = tokenizer.decode(encoded)\n",
    "pprint(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be done for many samples at the same time with the use of ``batch_`` prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = 32\n",
      "[11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n",
      "\n",
      "Encoded size = torch.Size([32, 19])\n",
      "\n",
      "32\n",
      "[11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "example_batch = tokenizer.convert_programs_to_tokens(train_progs[:32])\n",
    "print(f'Batch size = {len(example_batch)}')\n",
    "print([len(p) for p in example_batch])\n",
    "print()\n",
    "\n",
    "encoded = tokenizer.batch_encode(example_batch)\n",
    "print(f'Encoded size = {encoded.shape}')\n",
    "print()\n",
    "\n",
    "decoded = tokenizer.batch_decode(encoded)\n",
    "print(len(decoded))\n",
    "print([len(p) for p in decoded])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same directly from ``ProgramNode`` representation by using the ``_program`` suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program:\n",
      "[(0): scene(),\n",
      " (1): filter{size}[large](0),\n",
      " (2): filter{color}[gray](1),\n",
      " (3): filter{shape}[cylinder](2),\n",
      " (4): unique(3),\n",
      " (5): relate[right](4),\n",
      " (6): filter{size}[large](5),\n",
      " (7): filter{material}[metal](6),\n",
      " (8): unique(7),\n",
      " (9): relate[left](8),\n",
      " (10): filter{color}[brown](9),\n",
      " (11): filter{shape}[cube](10),\n",
      " (12): count(11)]\n",
      "Encoded ids:\n",
      "tensor([ 1,  4, 21, 12, 35, 43, 19, 24, 36, 43, 22, 14, 24, 41,  2])\n",
      "\n",
      "Raw conversion to programs:\n",
      "[(0): <START>(-1),\n",
      " (1): count(0),\n",
      " (2): filter{shape}[cube](1),\n",
      " (3): filter{color}[brown](2),\n",
      " (4): relate[left](3),\n",
      " (5): unique(4),\n",
      " (6): filter{material}[metal](5),\n",
      " (7): filter{size}[large](6),\n",
      " (8): relate[right](7),\n",
      " (9): unique(8),\n",
      " (10): filter{shape}[cylinder](9),\n",
      " (11): filter{color}[gray](10),\n",
      " (12): filter{size}[large](11),\n",
      " (13): scene(),\n",
      " (14): <END>(13)]\n",
      "\n",
      "Decoded programs:\n",
      "[(0): scene(),\n",
      " (1): filter{size}[large](0),\n",
      " (2): filter{color}[gray](1),\n",
      " (3): filter{shape}[cylinder](2),\n",
      " (4): unique(3),\n",
      " (5): relate[right](4),\n",
      " (6): filter{size}[large](5),\n",
      " (7): filter{material}[metal](6),\n",
      " (8): unique(7),\n",
      " (9): relate[left](8),\n",
      " (10): filter{color}[brown](9),\n",
      " (11): filter{shape}[cube](10),\n",
      " (12): count(11)]\n",
      "\n",
      "32 [11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n",
      "torch.Size([32, 19])\n",
      "32 [11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n"
     ]
    }
   ],
   "source": [
    "print('Example program:')\n",
    "example_program = train_progs[1312]\n",
    "pprint(example_program)\n",
    "\n",
    "print('Encoded ids:')\n",
    "encoded = tokenizer.encode_program(example_program)\n",
    "print(encoded)\n",
    "print()\n",
    "print('Raw conversion to programs:')\n",
    "pprint(tokenizer.convert_ids_to_programs([encoded.tolist()])[0])\n",
    "print()\n",
    "print('Decoded programs:')\n",
    "decoded = tokenizer.decode_program(encoded)\n",
    "pprint(decoded)\n",
    "print()\n",
    "\n",
    "# and same for multiple sample adding batch prefix\n",
    "example_batch = train_progs[:32]\n",
    "print(len(example_batch), [len(p) for p in example_batch])\n",
    "encoded = tokenizer.batch_encode_program(example_batch)\n",
    "decoded = tokenizer.batch_decode_program(encoded)\n",
    "print(encoded.shape)\n",
    "print(len(decoded), [len(p) for p in decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the programs to token IDs and save the checkpoint datasets for both train and val set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "save_dir = os.path.join(root, \"checkpoints/clevr_programs/v0\")\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "with open(os.path.join(save_dir, 'vocab.json'), 'w') as f:\n",
    "    json.dump(tokenizer.vocab, f)\n",
    "\n",
    "train_prog_ids = tokenizer.batch_encode_program(train_progs)\n",
    "np.save(os.path.join(save_dir,'train_ids.npy' ), train_prog_ids.numpy())\n",
    "    \n",
    "_ds = json.load(open(val_questions_path))['questions']\n",
    "_progs = formalize_program_annots(_ds)\n",
    "_progs = tokenizer.batch_encode_program(_progs)\n",
    "np.save(os.path.join(save_dir,'val_ids.npy'), _progs.numpy())\n",
    "\n",
    "del _ds, _progs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v.1) Vocab-agnostic Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same as before but use the special tokens ``[, ]`` to define specific concept values as arguments, untangled from primitives, in order to reach the *vocab-agnostic* version of the library that we mention above. For a learning system, the specific concept values will be chosen from the input query, therefore freeing the primitives library from specific vocabulary. This will hopefully enable generalization,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<END>': 2,\n",
      " '<PAD>': 0,\n",
      " '<START>': 1,\n",
      " '<UNK>': 3,\n",
      " '<[>': 6,\n",
      " '<]>': 7,\n",
      " 'count': 8,\n",
      " 'equal_integer': 9,\n",
      " 'equal{color}': 10,\n",
      " 'equal{material}': 11,\n",
      " 'equal{shape}': 12,\n",
      " 'equal{size}': 13,\n",
      " 'exist': 14,\n",
      " 'filter{color}': 15,\n",
      " 'filter{material}': 16,\n",
      " 'filter{shape}': 17,\n",
      " 'filter{size}': 18,\n",
      " 'greater_than': 19,\n",
      " 'intersect': 20,\n",
      " 'less_than': 21,\n",
      " 'query{color}': 22,\n",
      " 'query{material}': 23,\n",
      " 'query{shape}': 24,\n",
      " 'query{size}': 25,\n",
      " 'relate': 26,\n",
      " 'same{color}': 27,\n",
      " 'same{material}': 28,\n",
      " 'same{shape}': 29,\n",
      " 'same{size}': 30,\n",
      " 'scene': 31,\n",
      " 'union': 32,\n",
      " 'unique': 33}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ProgramTokenizer(version=1)\n",
    "tokenizer.make_from_dataset(train_progs)\n",
    "#Vocab = json.load(open('checkpoints/clevr_programs/CLEVR_vocabularies.json'))['prog2id']\n",
    "#tokenizer.make_vocab(Vocab)\n",
    "\n",
    "Vocab = tokenizer.vocab\n",
    "pprint(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the two extra special tokens, that signature the beginning and end of a concept value argument. Our primitives are now free of the domain vocabulary!\n",
    "\n",
    "Let's do the same as before to see the structure of tokens, encode-decode some programs and save train-val checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program:\n",
      "[(0): scene(),\n",
      " (1): filter{size}[small](0),\n",
      " (2): filter{color}[yellow](1),\n",
      " (3): filter{material}[metal](2),\n",
      " (4): unique(3),\n",
      " (5): same{shape}(4),\n",
      " (6): exist(5)]\n",
      "\n",
      "Example program tokens:\n",
      "['scene',\n",
      " 'filter{size}',\n",
      " '<[>',\n",
      " 'small',\n",
      " '<]>',\n",
      " 'filter{color}',\n",
      " '<[>',\n",
      " 'yellow',\n",
      " '<]>',\n",
      " 'filter{material}',\n",
      " '<[>',\n",
      " 'metal',\n",
      " '<]>',\n",
      " 'unique',\n",
      " 'same{shape}',\n",
      " 'exist']\n",
      "\n",
      "Encoded ids:\n",
      "tensor([ 1, 14, 29, 33,  7,  3,  6, 16,  7,  3,  6, 15,  7,  3,  6, 18, 31,  2])\n",
      "\n",
      "Raw conversion to tokens:\n",
      "['<START>',\n",
      " 'exist',\n",
      " 'same{shape}',\n",
      " 'unique',\n",
      " '<]>',\n",
      " '<UNK>',\n",
      " '<[>',\n",
      " 'filter{material}',\n",
      " '<]>',\n",
      " '<UNK>',\n",
      " '<[>',\n",
      " 'filter{color}',\n",
      " '<]>',\n",
      " '<UNK>',\n",
      " '<[>',\n",
      " 'filter{size}',\n",
      " 'scene',\n",
      " '<END>']\n",
      "\n",
      "Decoded tokens:\n",
      "['scene',\n",
      " 'filter{size}',\n",
      " '<[>',\n",
      " '<UNK>',\n",
      " '<]>',\n",
      " 'filter{color}',\n",
      " '<[>',\n",
      " '<UNK>',\n",
      " '<]>',\n",
      " 'filter{material}',\n",
      " '<[>',\n",
      " '<UNK>',\n",
      " '<]>',\n",
      " 'unique',\n",
      " 'same{shape}',\n",
      " 'exist']\n"
     ]
    }
   ],
   "source": [
    "print('Example program:')\n",
    "example_program = train_progs[1231]\n",
    "pprint(example_program)\n",
    "print()\n",
    "\n",
    "print('Example program tokens:')\n",
    "example_tokens = tokenizer._tokenize(example_program)\n",
    "pprint(example_tokens)\n",
    "print()\n",
    "\n",
    "print('Encoded ids:')\n",
    "encoded = tokenizer.encode(example_tokens)\n",
    "print(encoded)\n",
    "print()\n",
    "print('Raw conversion to tokens:')\n",
    "pprint(tokenizer.convert_ids_to_tokens([encoded.tolist()])[0])\n",
    "print()\n",
    "print('Decoded tokens:')\n",
    "decoded = tokenizer.decode(encoded)\n",
    "pprint(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example program:\n",
      "[(0): scene(),\n",
      " (1): filter{size}[large](0),\n",
      " (2): filter{color}[gray](1),\n",
      " (3): filter{shape}[cylinder](2),\n",
      " (4): unique(3),\n",
      " (5): relate[right](4),\n",
      " (6): filter{size}[large](5),\n",
      " (7): filter{material}[metal](6),\n",
      " (8): unique(7),\n",
      " (9): relate[left](8),\n",
      " (10): filter{color}[brown](9),\n",
      " (11): filter{shape}[cube](10),\n",
      " (12): count(11)]\n",
      "\n",
      "Example program tokens:\n",
      "['scene',\n",
      " 'filter{size}',\n",
      " '<[>',\n",
      " 'large',\n",
      " '<]>',\n",
      " 'filter{color}',\n",
      " '<[>',\n",
      " 'gray',\n",
      " '<]>',\n",
      " 'filter{shape}',\n",
      " '<[>',\n",
      " 'cylinder',\n",
      " '<]>',\n",
      " 'unique',\n",
      " 'relate',\n",
      " '<[>',\n",
      " 'right',\n",
      " '<]>',\n",
      " 'filter{size}',\n",
      " '<[>',\n",
      " 'large',\n",
      " '<]>',\n",
      " 'filter{material}',\n",
      " '<[>',\n",
      " 'metal',\n",
      " '<]>',\n",
      " 'unique',\n",
      " 'relate',\n",
      " '<[>',\n",
      " 'left',\n",
      " '<]>',\n",
      " 'filter{color}',\n",
      " '<[>',\n",
      " 'brown',\n",
      " '<]>',\n",
      " 'filter{shape}',\n",
      " '<[>',\n",
      " 'cube',\n",
      " '<]>',\n",
      " 'count']\n",
      "\n",
      "Encoded ids:\n",
      "tensor([ 1,  8,  7,  3,  6, 17,  7,  3,  6, 15,  7,  3,  6, 26, 33,  7,  3,  6,\n",
      "        16,  7,  3,  6, 18,  7,  3,  6, 26, 33,  7,  3,  6, 17,  7,  3,  6, 15,\n",
      "         7,  3,  6, 18, 31,  2])\n",
      "\n",
      "Raw conversion to programs:\n",
      "[(0): <START>(-1),\n",
      " (1): count(0),\n",
      " (2): <]>(1),\n",
      " (3): <UNK>(2),\n",
      " (4): <{shape}[>](3),\n",
      " (5): <UNK>(4),\n",
      " (6): <{color}[>](5),\n",
      " (7): <UNK>(6),\n",
      " (8): <[>](7),\n",
      " (9): <]>(8),\n",
      " (10): <UNK>(9),\n",
      " (11): <{material}[>](10),\n",
      " (12): <UNK>(11),\n",
      " (13): <{size}[>](12),\n",
      " (14): <UNK>(13),\n",
      " (15): <[>](14),\n",
      " (16): <]>(15),\n",
      " (17): <UNK>(16),\n",
      " (18): <{shape}[>](17),\n",
      " (19): <UNK>(18),\n",
      " (20): <{color}[>](19),\n",
      " (21): <UNK>(20),\n",
      " (22): <{size}[>](21),\n",
      " (23): <END>(22)]\n",
      "\n",
      "Decoded programs:\n",
      "[(0): scene(),\n",
      " (1): filter{size}(0),\n",
      " (2): <[>](1),\n",
      " (3): filter{color}(2),\n",
      " (4): <[>](3),\n",
      " (5): filter{shape}(4),\n",
      " (6): <[>](5),\n",
      " (7): unique(6),\n",
      " (8): relate(7),\n",
      " (9): <[>](8),\n",
      " (10): filter{size}(9),\n",
      " (11): <[>](10),\n",
      " (12): filter{material}(11),\n",
      " (13): <[>](12),\n",
      " (14): unique(13),\n",
      " (15): relate(14),\n",
      " (16): <[>](15),\n",
      " (17): filter{color}(16),\n",
      " (18): <[>](17),\n",
      " (19): filter{shape}(18),\n",
      " (20): <[>](19),\n",
      " (21): count(20)]\n",
      "\n",
      "32 [11, 7, 12, 15, 6, 16, 13, 8, 7, 10, 9, 7, 10, 9, 14, 16, 16, 12, 11, 17, 12, 15, 10, 14, 13, 8, 16, 14, 7, 7, 8, 6]\n",
      "torch.Size([32, 55])\n",
      "32 [17, 10, 17, 23, 9, 27, 21, 11, 10, 16, 14, 9, 16, 13, 23, 24, 26, 20, 15, 29, 16, 23, 17, 23, 22, 12, 26, 21, 9, 10, 12, 8]\n"
     ]
    }
   ],
   "source": [
    "# Directly from ProgramNode representations\n",
    "\n",
    "print('Example program:')\n",
    "example_program = train_progs[1312]\n",
    "pprint(example_program)\n",
    "print()\n",
    "\n",
    "print('Example program tokens:')\n",
    "example_tokens = tokenizer._tokenize(example_program)\n",
    "pprint(example_tokens)\n",
    "print()\n",
    "\n",
    "print('Encoded ids:')\n",
    "encoded = tokenizer.encode_program(example_program)\n",
    "print(encoded)\n",
    "print()\n",
    "\n",
    "print('Raw conversion to programs:')\n",
    "pprint(tokenizer.convert_ids_to_programs([encoded.tolist()])[0])\n",
    "print()\n",
    "\n",
    "print('Decoded programs:')\n",
    "decoded = tokenizer.decode_program(encoded)\n",
    "pprint(decoded)\n",
    "print()\n",
    "\n",
    "# and same for multiple sample adding batch prefix\n",
    "example_batch = train_progs[:32]\n",
    "print(len(example_batch), [len(p) for p in example_batch])\n",
    "encoded = tokenizer.batch_encode_program(example_batch)\n",
    "decoded = tokenizer.batch_decode_program(encoded)\n",
    "print(encoded.shape)\n",
    "print(len(decoded), [len(p) for p in decoded])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v.2) Concept-agnostic Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing CLEVR programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>', '<START>', '<END>', '<UNK>', '<]>', '<[>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<]>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.sva_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<[>', '<]>'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VALUE_ARGUMENT_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Tokens = List[str]\n",
    "    \n",
    "    def _convert_to_v0(tokens: List[Tokens]) -> List[Tokens]:\n",
    "        def _convert(ts: Tokens) -> Tokens:\n",
    "            output_sequence = []\n",
    "            i=0\n",
    "            while i < len(ts):\n",
    "                token = ts[i]\n",
    "                if tokenizer.version == 2:\n",
    "                    if token == tokenizer.sca_token:\n",
    "                        _concept = '{' + ts[i+1] + '}'\n",
    "                        i += 3\n",
    "                if token == tokenizer.sva_token:\n",
    "                    _value = '[' + ts[i+1] + ']'\n",
    "                    i += 3\n",
    "                else:\n",
    "                    _concept, _value = '', ''\n",
    "                    i += 1\n",
    "                output_sequence.append(token + _concept + _value)\n",
    "            return output_sequence\n",
    "        return list(map(_convert, tokens))\n",
    "\n",
    "tokenizer._convert_to_v0 = _convert_to_v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
